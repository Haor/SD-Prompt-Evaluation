"""
This script calculates the cosine similarity between two sets of prompts. 
The first set of prompts is extracted from the metadata of PNG images. 
The second set of prompts is generated by applying the WD 1.4 MOAT Tagger V2 model 
(https://huggingface.co/SmilingWolf/wd-v1-4-moat-tagger-v2) to the images.

Usage:
    python text_extraction_and_similarity.py

Output:
    The script will output a CSV file named 'results.csv' with the following columns:
    - 'PNG File': the name of the PNG file
    - 'Prompt A': the prompt extracted from the PNG file
    - 'Prompt B': the prompt generated by the WD 1.4 MOAT Tagger V2 model
    - 'Cosine Similarity': the cosine similarity between the embeddings of Prompt A and Prompt B

    The script will also print the average, maximum, and minimum cosine similarity.
"""
import os
import json
from concurrent.futures import as_completed, ThreadPoolExecutor
from typing import Dict, List, Tuple
from pathlib import Path

import png
import requests
import cv2
import numpy as np
import pandas as pd
from tqdm import tqdm
from tqdm.contrib.concurrent import process_map
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from onnxruntime import InferenceSession
from PIL import Image
from sklearn.metrics.pairwise import cosine_similarity
from huggingface_hub import hf_hub_download

folder_path = '/path/xxxx'
png_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]

API_KEY = 'sk-xxxxxx'

headers = {
    'Content-Type': 'application/json',
    'Authorization': f'Bearer {API_KEY}'
}

proxies = {
  "http": "http://127.0.0.1:8899",
  "https": "http://127.0.0.1:8899",
}

def get_embedding(text):
    data = {
        'input': text,
        'model': 'text-embedding-ada-002'
    }
    session = requests.Session()
    retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[ 500, 502, 503, 504 ])
    session.mount('https://', HTTPAdapter(max_retries=retries))
    
    response = requests.post('https://api.openai.com/v1/embeddings', headers=headers, data=json.dumps(data))
    #response = requests.post('https://api.openai.com/v1/embeddings', headers=headers, proxies=proxies, data=json.dumps(data))
    return response.json()['data'][0]['embedding']

def get_embeddings(prompts, desc=None):
    with ThreadPoolExecutor(max_workers=10) as executor:
        embeddings = list(tqdm(executor.map(get_embedding, prompts), total=len(prompts), desc=desc))
    return embeddings

def get_png_text(filename):
    with open(filename, 'rb') as f:
        r = png.Reader(file=f)
        chunks = r.chunks()
        text_chunks = [(chunk_type, data) for chunk_type, data in chunks if chunk_type in [b'tEXt', b'zTXt', b'iTXt']]
    return text_chunks

prompts_A = []
for png_file in tqdm(png_files, desc="Getting Prompt A"):
    file_path = os.path.join(folder_path, png_file)
    text_chunks = get_png_text(file_path)
    parameters = None
    for chunk_type, data in text_chunks:
        if chunk_type == b'tEXt' and b'parameters' in data:
            parameters = data.decode('ISO-8859-1').split('\x00')[1]
            break
    if parameters is None:
        print(f"No parameters found in file {png_file}, skipping this file.")
        continue
    prompt_A = parameters.split('Negative prompt:')[0].strip()
    prompts_A.append(prompt_A)


def make_square(img, target_size):
    old_size = img.shape[:2]
    desired_size = max(old_size)
    desired_size = max(desired_size, target_size)

    delta_w = desired_size - old_size[1]
    delta_h = desired_size - old_size[0]
    top, bottom = delta_h // 2, delta_h - (delta_h // 2)
    left, right = delta_w // 2, delta_w - (delta_w // 2)

    color = [255, 255, 255]
    new_im = cv2.copyMakeBorder(
        img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color
    )
    return new_im

def smart_resize(img, size):
    # Assumes the image has already gone through make_square
    if img.shape[0] > size:
        img = cv2.resize(img, (size, size), interpolation=cv2.INTER_AREA)
    elif img.shape[0] < size:
        img = cv2.resize(img, (size, size), interpolation=cv2.INTER_CUBIC)
    return img

class WDInterrogator():
    def __init__(
        self,
        name: str,
        model_path="model.onnx",
        tags_path="selected_tags.csv",
        **kwargs,
    ) -> None:
        self.name = name
        self.model_path = model_path
        self.tags_path = tags_path
        self.kwargs = kwargs
        
    def load(self):
        model_path, tags_path = self.download()
        self.model = InferenceSession(str(model_path), providers=["CUDAExecutionProvider"])
        self.tags = pd.read_csv(tags_path)
        self.useless_tags = set(
            ['virtual_youtuber'] +
            [tag for tag in self.tags if 'alternate_' in tag] +
            ['genderswap', 'genderswap_(mtf)', 'genderswap_(ftm)', 'ambiguous_gender']
        )
        print(f"Loaded {self.name} model from {model_path}")
    
    def download(self):
        print(f"Loading {self.name} model file from {self.kwargs['repo_id']}")
        model_path = Path(hf_hub_download(**self.kwargs, filename=self.model_path))
        tags_path = Path(hf_hub_download(**self.kwargs, filename=self.tags_path))
        return model_path, tags_path
    
    def postprocess_tags(
        self,
        tags: Dict[str, float],
        threshold=0.35
    ) -> Dict[str, float]:
        tags = {
            t: c
            for t, c in sorted(
                tags.items(),
                key=lambda i: i[1],
                reverse=True,
            )
            if (c >= threshold and t not in self.useless_tags)
        }
        return tags

    def interrogate(self, image):
        if not hasattr(self, 'model') or self.model is None:
            self.load()
            
        _, height, _, _ = self.model.get_inputs()[0].shape
        image = image.convert("RGBA")
        new_image = Image.new("RGBA", image.size, "WHITE")
        new_image.paste(image, mask=image)
        image = new_image.convert("RGB")
        image = np.asarray(image)
        image = image[:, :, ::-1]
        image = make_square(image, height)
        image = smart_resize(image, height)
        image = image.astype(np.float32)
        image = np.expand_dims(image, 0)

        input_name = self.model.get_inputs()[0].name
        label_name = self.model.get_outputs()[0].name
        confidents = self.model.run([label_name], {input_name: image})[0]

        tags = self.tags[:][["name"]]
        tags["confidents"] = confidents[0]

        ratings = dict(tags[:4].values)
        tags = dict(tags[4:].values)
        return ratings, tags

interrogator = WDInterrogator(
    "wd-v1-4-moat-tagger-v2",
    repo_id="SmilingWolf/wd-v1-4-moat-tagger-v2"
)

prompts_B = []
processed_files = []

for png_file, prompt_A in tqdm(zip(png_files, prompts_A), total=len(png_files), desc="Getting Prompt B"):
    try:
        img_path = os.path.join(folder_path, png_file)
        ratings, tags = interrogator.interrogate(Image.open(img_path))
        tags = interrogator.postprocess_tags(tags, threshold=0.5)
        prompt_B = ", ".join(tags.keys())
    except Exception as e:
        print(f"Error processing file {png_file}: {e}")
        prompt_B = ''
    finally:
        prompts_B.append(prompt_B)
        processed_files.append(png_file)

embeddings_A = get_embeddings(prompts_A, desc="Getting embeddings for Prompt A")
embeddings_B = get_embeddings(prompts_B, desc="Getting embeddings for Prompt B")

similarities = []
for prompt_A_vector, prompt_B_vector in tqdm(zip(embeddings_A, embeddings_B), total=len(embeddings_A), desc="Calculating cosine similarity"):
    cosine_sim = cosine_similarity([prompt_A_vector], [prompt_B_vector])
    similarities.append(cosine_sim[0][0])

results = pd.DataFrame({
    'PNG File': processed_files,
    'Prompt A': prompts_A,
    'Prompt B': prompts_B,
    'Cosine Similarity': similarities
})

results.to_csv('results.csv', index=False)
print(results)

average_similarity = results['Cosine Similarity'].mean()
print(f"Average Cosine Similarity: {average_similarity}")

max_similarity = results['Cosine Similarity'].max()
print(f"Maximum Cosine Similarity: {max_similarity}")

min_similarity = results['Cosine Similarity'].min()
print(f"Minimum Cosine Similarity: {min_similarity}")
